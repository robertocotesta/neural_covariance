{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/robertocotesta/neural_covariance.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyiLMwRglaC0",
        "outputId": "8c7cac2a-60aa-4b24-f96c-cd93a0f91276"
      },
      "id": "WyiLMwRglaC0",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'neural_covariance' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a704ab3e",
      "metadata": {
        "id": "a704ab3e"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2322d480",
      "metadata": {
        "id": "2322d480"
      },
      "source": [
        "## Data wrangling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bb3215ec",
      "metadata": {
        "id": "bb3215ec"
      },
      "outputs": [],
      "source": [
        "param_list = ['Mc', 'eta', 'DL', 'tc', 'phic', 'iota', 'ra', 'dec', 'psi']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "34b4d5b6",
      "metadata": {
        "id": "34b4d5b6"
      },
      "outputs": [],
      "source": [
        "def recreate_symmetric_matrix(size_X,X_flatten):\n",
        "    X = np.zeros((size_X,size_X))\n",
        "    X[np.triu_indices(X.shape[0], k = 0)] = X_flatten\n",
        "    X = X + X.T - np.diag(np.diag(X))\n",
        "    return X\n",
        "\n",
        "def flatten_symmetric_matrix(X):\n",
        "    return X[np.triu_indices(X.shape[0], k = 0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1e6d1943",
      "metadata": {
        "id": "1e6d1943"
      },
      "outputs": [],
      "source": [
        "with open('/content/neural_covariance/Data/input.pkl','rb') as f:\n",
        "    input_dataset = pickle.load(f)\n",
        "with open('/content/neural_covariance/Data/output.pkl','rb') as f:\n",
        "    output_dataset = pickle.load(f)    \n",
        "with open('/content/neural_covariance/Data/SNRs.pkl','rb') as f:\n",
        "    SNR_dataset = pickle.load(f)      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "af09a99c",
      "metadata": {
        "id": "af09a99c"
      },
      "outputs": [],
      "source": [
        "input_df = pd.DataFrame.from_dict(input_dataset, orient='index',columns=param_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "247edd6a",
      "metadata": {
        "id": "247edd6a"
      },
      "outputs": [],
      "source": [
        "# Each matrix is symmetric, hence I only extract and flatten its triangular upper part\n",
        "output_df = pd.DataFrame(data=[flatten_symmetric_matrix(output_dataset[i]) for i in output_dataset.keys()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "054e4499",
      "metadata": {
        "id": "054e4499"
      },
      "outputs": [],
      "source": [
        "# Load SNR for interpolation\n",
        "SNR_df = pd.DataFrame(data=SNR_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cd3d14ac",
      "metadata": {
        "id": "cd3d14ac"
      },
      "outputs": [],
      "source": [
        "# Split test, train, CV for Fisher\n",
        "# X_train, X_test, y_train, y_test = train_test_split(scaled_input_df,output_df.values,test_size=0.4,random_state=0)\n",
        "# X_CV, X_test, y_CV, y_test = train_test_split(X_test,y_test,test_size=0.5,random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0] == X_train[2]"
      ],
      "metadata": {
        "id": "ePRLDg-132wC",
        "outputId": "6745bb8a-e13c-4ddd-fb3b-52f4b98a5474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "id": "ePRLDg-132wC",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7879b9ec1a00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[:,4]"
      ],
      "metadata": {
        "id": "mSOBF19R51BH",
        "outputId": "4c0912c6-3ccb-49f1-c3df-5b20dd38a033",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mSOBF19R51BH",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9be3283c",
      "metadata": {
        "id": "9be3283c"
      },
      "outputs": [],
      "source": [
        "# Split test, train, CV for SNR\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_df.values,SNR_df.values,test_size=0.4,random_state=0)\n",
        "X_CV, X_test, y_CV, y_test = train_test_split(X_test,y_test,test_size=0.5,random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i_max = 2000\n",
        "# X_train, y_train = X_train[:i_max], y_train[:i_max]"
      ],
      "metadata": {
        "id": "MSWey9Pz4LBW"
      },
      "id": "MSWey9Pz4LBW",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sc_Input = StandardScaler()\n",
        "# sc_Output = StandardScaler()\n",
        "# input_df = input_df[['Mc', 'eta', 'DL', 'iota', 'ra', 'dec']]\n",
        "# # Scaling input\n",
        "# scaled_input_df = sc_Input.fit_transform(input_df)\n",
        "# # Scaling output\n",
        "# scaled_output_df = sc_Output.fit_transform(output_df)"
      ],
      "metadata": {
        "id": "9VZBOfSyRC9e"
      },
      "id": "9VZBOfSyRC9e",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6936d511",
      "metadata": {
        "id": "6936d511"
      },
      "source": [
        "## Build interpolant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fe13090f",
      "metadata": {
        "id": "fe13090f"
      },
      "outputs": [],
      "source": [
        "# Definition of the device for training\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# General settings for the training\n",
        "epochs = 1000\n",
        "lr = 0.001\n",
        "batch_size  = y_train.shape[0]\n",
        "input_layer_dim = X_train.shape[1]\n",
        "output_layer_dim = y_train.shape[1]"
      ],
      "metadata": {
        "id": "Qxt8qXDLCSiu"
      },
      "id": "Qxt8qXDLCSiu",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "15851cb6",
      "metadata": {
        "id": "15851cb6"
      },
      "outputs": [],
      "source": [
        "# Definition of the neural network\n",
        "class interpolant(nn.Module):\n",
        "    def __init__(self,dim_input,dim_output):\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(dim_input, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, dim_output),\n",
        "        )\n",
        "        \n",
        "    def forward(self,X):\n",
        "        return self.linear_relu_stack(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a57675d8",
      "metadata": {
        "id": "a57675d8"
      },
      "outputs": [],
      "source": [
        "model = interpolant(input_layer_dim,output_layer_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.from_numpy(X_train).float()\n",
        "y_train = torch.from_numpy(y_train).float()\n",
        "\n",
        "X_CV = torch.from_numpy(X_CV).float()\n",
        "y_CV = torch.from_numpy(y_CV).float()"
      ],
      "metadata": {
        "id": "kU4LzUT_sv0V"
      },
      "id": "kU4LzUT_sv0V",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_mean = X_train.mean(axis=0)\n",
        "X_train_std = X_train.std(axis=0)\n",
        "X_train_normalized = (X_train-X_train_mean)/X_train_std\n",
        "X_CV_normalized = (X_CV-X_train_mean)/X_train_std"
      ],
      "metadata": {
        "id": "dETdrytQ1efM"
      },
      "id": "dETdrytQ1efM",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_array.append(loss.item())\n",
        "\n",
        "    return loss_array"
      ],
      "metadata": {
        "id": "uLItLODlvcWl"
      },
      "id": "uLItLODlvcWl",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7dc8480c",
      "metadata": {
        "id": "7dc8480c"
      },
      "outputs": [],
      "source": [
        "# def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "#     loss_array = []\n",
        "#     size = len(dataloader.dataset)\n",
        "#     for batch, (X, y) in enumerate(dataloader):\n",
        "#         X = X.to(device)\n",
        "#         y = y.to(device)\n",
        "#         # Compute prediction and loss\n",
        "#         pred = model(X)\n",
        "#         loss = loss_fn(pred, y)\n",
        "\n",
        "#         # Backpropagation\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         loss_array.append(loss.item())\n",
        "\n",
        "#     return loss_array\n",
        "\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:.2E}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "# def test_loop(dataloader, model, loss_fn):\n",
        "#     size = len(dataloader.dataset)\n",
        "#     num_batches = len(dataloader)\n",
        "#     test_loss, correct = 0, 0\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for X, y in dataloader:\n",
        "#             X = X.to(device)\n",
        "#             y = y.to(device)\n",
        "#             pred = model(X)\n",
        "#             test_loss += loss_fn(pred, y).item()\n",
        "\n",
        "#     test_loss /= num_batches\n",
        "#     print(f\"Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a9742ab3",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9742ab3",
        "outputId": "455f0d0b-dbed-413b-f176-4bab373425ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:01<00:00, 848.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Definition of loss function and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
        "loss_array = []\n",
        "X = X_train_normalized.to(device)\n",
        "y = y_train.to(device)\n",
        "# Training loop\n",
        "for t in tqdm(range(epochs)):\n",
        "    #print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    # Compute prediction and loss\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_array.append(loss.item())\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(np.arange(epochs),loss_array)\n",
        "plt.yscale('log')"
      ],
      "metadata": {
        "id": "BbY8gyL4odf4",
        "outputId": "0a53fa67-2814-4677-e5d3-e0d74cd2db10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "id": "BbY8gyL4odf4",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAI/CAYAAAC1XpeNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAASN0lEQVR4nO3db6imeV3H8c83F6M/cPxvtmvNlouxEiQMK1GBlH/WB6OiPnCfJLE1BPmgImhFyDIJi0qJ1BhSlAhtkaId3BKzRAipXU3IzWwnTdzNcnXlgEku2q8H55aO26x7ds5n5j5z5vWC4cx9Xdd97/fMj7PnPdd1nXtmrRUAAA7vW7Y9AADAcSGsAABKhBUAQImwAgAoEVYAACXCCgCg5KptD5AkT3jCE9aJEye2PQYAwMP68Ic//Pm11hPPt+9IhNWJEydy5513bnsMAICHNTOffqh9LgUCAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUFIPq5n5vpl568y8u/3aAABH2YHCambeNjOfm5mPPWj7jTPziZk5NzO3JMla65NrrZsvxrAAAEfZQc9YvT3Jjfs3zMyjkrwpyQuSXJ/kppm5vjodAMBl5EBhtdb6YJL7H7T5hiTnNmeoHkjyriQvKs8HAHDZOMw9Vlcn+cy+x/ckuXpmHj8zf5DkmTPzqod68sycnpk7Z+bO++677xBjAAAcDVe1X3Ct9YUkP3uA484kOZMkJ0+eXO05AAAutcOcsbo3yVP3Pb5msw0A4Ip0mLC6I8l1M3PtzDw6ycuT3NYZCwDg8nPQt1t4Z5IPJXn6zNwzMzevtb6a5JVJ3pvk40luXWvddfFGBQA42g50j9Va66aH2H57kturEwEAXKb8kzYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJVsNq5k5NTNndnd3tzkGAEDFVsNqrXV2rXV6Z2dnm2MAAFS4FAgAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlGw1rGbm1Myc2d3d3eYYAAAVWw2rtdbZtdbpnZ2dbY4BAFDhUiAAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAo2WpYzcypmTmzu7u7zTEAACq2GlZrrbNrrdM7OzvbHAMAoMKlQACAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUbDWsZubUzJzZ3d3d5hgAABVbDau11tm11umdnZ1tjgEAUOFSIABAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJRsNaxm5tTMnNnd3d3mGAAAFVsNq7XW2bXW6Z2dnW2OAQBQ4VIgAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAkq2G1cycmpkzu7u72xwDAKBiq2G11jq71jq9s7OzzTEAACpcCgQAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBkq2E1M6dm5szu7u42xwAAqNhqWK21zq61Tu/s7GxzDACACpcCAQBKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoGSrYTUzp2bmzO7u7jbHAACo2GpYrbXOrrVO7+zsbHMMAIAKlwIBAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAoEVYAACXCCgCgRFgBAJQIKwCAEmEFAFAirAAASoQVAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgJKr2i84M9+R5M1JHkjygbXWH7f/GwAAR9GBzljNzNtm5nMz87EHbb9xZj4xM+dm5pbN5pckefda62eSvLA8LwDAkXXQS4FvT3Lj/g0z86gkb0rygiTXJ7lpZq5Pck2Sz2wO+1pnTACAo+9AYbXW+mCS+x+0+YYk59Zan1xrPZDkXUlelOSe7MXVgV8fAOA4OEz4XJ3/OzOV7AXV1Un+NMlLZ+YtSc4+1JNn5vTM3Dkzd953332HGAMA4Gio37y+1vqvJD91gOPOJDmTJCdPnlztOQAALrXDnLG6N8lT9z2+ZrMNAOCKdJiwuiPJdTNz7cw8OsnLk9zWGQsA4PJz0LdbeGeSDyV5+szcMzM3r7W+muSVSd6b5ONJbl1r3XXxRgUAONoOdI/VWuumh9h+e5LbqxMBAFymvB0CAECJsAIAKBFWAAAlwgoAoERYAQCUCCsAgBJhBQBQIqwAAEqEFQBAibACACgRVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQMlWw2pmTs3Mmd3d3W2OAQBQsdWwWmudXWud3tnZ2eYYAAAVs9ba9gyZmfuSfHrbc1xGnpDk89segv/Huhw91uTosSZHk3V5ZL53rfXE8+04EmHFIzMzd661Tm57Dr6RdTl6rMnRY02OJuvS4+Z1AIASYQUAUCKsLk9ntj0A52Vdjh5rcvRYk6PJupS4xwoAoMQZKwCAEmF1RM3M42bmfTNz9+bjYx/iuFdsjrl7Zl5xnv23zczHLv7Ex99h1mRmvn1m3jMz/zwzd83M6y/t9MfPzNw4M5+YmXMzc8t59n/rzPzJZv/fzcyJfftetdn+iZl5/iUd/Bi70DWZmefOzIdn5h83H3/8kg9/jB3ma2Wz/3tm5ksz80uXbOjLmLA6um5J8v611nVJ3r95/A1m5nFJXpPkWUluSPKa/d/sZ+YlSb50aca9Ihx2TX57rfUDSZ6Z5Edm5gWXZuzjZ2YeleRNSV6Q5PokN83M9Q867OYkX1xrPS3JG5L85ua51yd5eZJnJLkxyZs3r8chHGZNsvf+SafWWj+Y5BVJ/ujSTH38HXJdvu53k/zFxZ71uBBWR9eLkrxj8/t3JHnxeY55fpL3rbXuX2t9Mcn7sveNIjPznUl+McnrLv6oV4wLXpO11pfXWn+TJGutB5J8JMk1F3/kY+uGJOfWWp/c/Hm+K3vrs9/+9Xp3kp+Ymdlsf9da6ytrrU8lObd5PQ7ngtdkrfUPa61/32y/K8m3zcy3XpKpj7/DfK1kZl6c5FPZWxcOQFgdXU9ea3128/v/SPLk8xxzdZLP7Ht8z2Zbkvx6kt9J8uWLNuGV57BrkiSZmcckOZW9s15cmIf9c95/zFrrq0l2kzz+gM/lkTvMmuz30iQfWWt95SLNeaW54HXZ/AX9l5P82iWY89i4atsDXMlm5q+SfNd5dr16/4O11pqZA//45sz8UJLvX2v9woOvlfPNXaw12ff6VyV5Z5LfW2t98sKmhONpZp6RvctQz9v2LCRJfjXJG9ZaX9qcwOIAhNUWrbWe81D7ZuY/Z+Ypa63PzsxTknzuPIfdm+TZ+x5fk+QDSX44ycmZ+bfsrfGTZuYDa61nh2/qIq7J151Jcvda642Hn/aKdm+Sp+57fM1m2/mOuWcTtDtJvnDA5/LIHWZNMjPXJPmzJD+51vrXiz/uFeMw6/KsJC+bmd9K8pgk/zMz/73W+v2LPvVlzKXAo+u27N3Emc3HPz/PMe9N8ryZeezmBunnJXnvWusta63vXmudSPKjSf5FVFVc8Jokycy8Lnv/w/r5iz/qsXdHkutm5tqZeXT2bka/7UHH7F+vlyX567X3xn23JXn55iehrk1yXZK/v0RzH2cXvCaby+PvSXLLWutvL9XAV4gLXpe11o+ttU5svpe8MclviKqHJ6yOrtcnee7M3J3kOZvHmZmTM/OHSbLWuj9791Ldsfn12s02Lo4LXpPN38Zfnb2fyvnIzHx0Zn56G5/EcbC5D+SV2YvWjye5da1118y8dmZeuDnsrdm7T+Rc9n6Q45bNc+9KcmuSf0ryl0l+bq31tUv9ORw3h1mTzfOeluRXNl8bH52ZJ13iT+FYOuS6cAG88zoAQIkzVgAAJcIKAKBEWAEAlAgrAIASYQUAUCKsAABKhBUAQImwAgAo+V9oahUMmyV0sgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_normalized"
      ],
      "metadata": {
        "id": "kNNFZ4st3p9A",
        "outputId": "e0876468-1769-495c-fc38-b472ae5b7e5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kNNFZ4st3p9A",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.1112, -0.1810, -0.9258,  ..., -0.0389, -1.2401,     inf],\n",
              "        [-1.3746, -1.1530, -0.9258,  ..., -0.0389, -1.2401,     inf],\n",
              "        [ 1.2312,  0.3762,  0.9728,  ..., -0.0389,  1.2094,     inf],\n",
              "        ...,\n",
              "        [-0.2850,  0.9228, -0.9258,  ..., -0.0389,  1.2094,     inf],\n",
              "        [ 0.4236,  0.3502, -0.9258,  ...,  1.2223,  1.2094,     inf],\n",
              "        [ 2.4765,  1.2984,  2.4112,  ..., -1.2198,  1.2094,     inf]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(X)"
      ],
      "metadata": {
        "id": "7FgtYhRapjqK",
        "outputId": "039bdda2-ff3b-4a9e-8a78-7af469d3406f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7FgtYhRapjqK",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_CV"
      ],
      "metadata": {
        "id": "kwa6zt7jz1iU",
        "outputId": "857606be-d4dc-46be-e884-5064fb4f5e1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kwa6zt7jz1iU",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2036.0526],\n",
              "        [ 278.7384],\n",
              "        [  40.8894],\n",
              "        ...,\n",
              "        [  13.5583],\n",
              "        [  48.1667],\n",
              "        [1253.8661]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8vNL4Kqmz22m"
      },
      "id": "8vNL4Kqmz22m",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}